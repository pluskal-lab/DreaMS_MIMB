{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This notebook demonstrates how to fine‑tune the DreaMS model for a binary classification task (detecting chlorine in molecules) using the MassSpecGym dataset. We’ll:\n",
    "\n",
    "1. **Annotate** the MassSpecGym MGF with chlorine labels.\n",
    "2. **Prepare** a `ChlorineDetectionDataset` and a `BenchmarkDataModule`.\n",
    "3. **Train** a baseline MLP classifier.\n",
    "4. **Fine‑tune** the DreaMS encoder with a classification head (`LitDreamsClassifier`).\n",
    "5. **Evaluate** on the test split and **save** the checkpoint.\n",
    "\n",
    "All paths are defined relative to `PROJECT_ROOT` for reproducibility."
   ],
   "id": "6f994e7db019d6e1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:21:34.641577Z",
     "start_time": "2025-07-05T09:21:34.637091Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# assume this notebook lives in notebooks/, so parent() is the repo root\n",
    "sys.path.append(str(Path().resolve().parent))\n",
    "from paths import PROJECT_ROOT\n",
    "\n",
    "from benchmark.utils.data import annotate_mgf_with_label\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from massspecgym.data.transforms import SpecTokenizer\n",
    "from benchmark.data.datasets import ChlorineDetectionDataset\n",
    "from benchmark.data.data_module import BenchmarkDataModule\n",
    "\n",
    "from benchmark.models.lit_dreams_module import LitDreamsClassifier\n",
    "\n",
    "\n"
   ],
   "id": "3b2a8a7b24af8ec6",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:08:40.874998Z",
     "start_time": "2025-07-05T09:08:40.870525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Paths\n",
    "DATA_DIR   = PROJECT_ROOT / \"data\" / \"massspecgym\"\n",
    "ORIG_MGF   = DATA_DIR / \"MassSpecGym.mgf\"\n",
    "LABELED_MGF = DATA_DIR / \"MassSpecGym_chlorine.mgf\"\n",
    "\n",
    "MODEL_PATH = PROJECT_ROOT / \"data\" / \"model_checkpoints\" / \"ssl_model.ckpt\""
   ],
   "id": "f4747f0c98adf205",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Here we define function for annotation of our data. It is important as it will set ground truth for our data. Here we are working with MassSpecGym data where each mass spectra is annotated with correct molecule and based on this we can further annotate our spectra.\n",
    "\n",
    "#### therefore here is are solving chlorine detection problem, we pull molecule associated with mass spectra and ask if molecule contains Chlorine, if yes we assign mass spectra label with value 1.0, respectively 0.0 if it does not contain Chlorine,"
   ],
   "id": "b1ff67a88dc37daf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:07:24.194953Z",
     "start_time": "2025-07-05T09:07:24.193263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define labeling function: 1.0 if 'Cl' in FORMULA\n",
    "label_fn = lambda md: float(\"Cl\" in md.get(\"FORMULA\", \"\"))"
   ],
   "id": "4750536a65954df0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:08:23.729907Z",
     "start_time": "2025-07-05T09:07:26.038433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Write out labeled MGF\n",
    "annotate_mgf_with_label(ORIG_MGF, LABELED_MGF, label_fn)\n",
    "print(f\"Labeled MGF written to: {LABELED_MGF}\")"
   ],
   "id": "4c24dfecc70d4b48",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labeled MGF written to: /Users/macbook/CODE/DreaMS_MIMB/data/massspecgym/MassSpecGym_chlorine.mgf\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Now as we prepared data, we can provide it to DreaMS and learns it to distinguish if mass spectra contain chlorine.",
   "id": "76b792fdadf24276"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:09:00.569312Z",
     "start_time": "2025-07-05T09:08:58.434517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1) Instantiate your Lightning module, pointing to the SSL checkpoint\n",
    "lit = LitDreamsClassifier(\n",
    "    ckpt_path=MODEL_PATH,\n",
    "    n_highest_peaks=128,    # must match our tokenizer\n",
    "    lr=1e-4,\n",
    "    dropout=0.1,\n",
    "    train_encoder=True     # fine-tune the entire encoder\n",
    ")"
   ],
   "id": "a4dac09464054714",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:09:03.510689Z",
     "start_time": "2025-07-05T09:09:03.508247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2) Tokenize spectra into fixed-length set representations\n",
    "spec_transform = SpecTokenizer(n_peaks=128)"
   ],
   "id": "83ed4f9edc766c3",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Here we actually load benchmark and data that will go into model, and will be trained on",
   "id": "90900bf81340cb13"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:09:45.217382Z",
     "start_time": "2025-07-05T09:09:10.059351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds_dreams = ChlorineDetectionDataset(\n",
    "    pth=LABELED_MGF,\n",
    "    spec_transform=spec_transform,\n",
    "    dtype=torch.float32\n",
    ")"
   ],
   "id": "329923f146d5d321",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:09:45.348663Z",
     "start_time": "2025-07-05T09:09:45.220034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3) Prepare the DataModule\n",
    "dm_dreams = BenchmarkDataModule(\n",
    "    dataset    = ds_dreams,\n",
    "    batch_size = 16,\n",
    "    num_workers= 0\n",
    ")\n",
    "dm_dreams.setup()"
   ],
   "id": "1ad524763368988b",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:09:49.138478Z",
     "start_time": "2025-07-05T09:09:49.113534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Inspect one batch just to sanity-check shapes:\n",
    "batch = next(iter(dm_dreams.train_dataloader()))\n",
    "print(\"spec shape:\", batch[\"spec\"].shape)    # -> [B, 61, 2]\n",
    "print(\"label shape:\", batch[\"label\"].shape)"
   ],
   "id": "dff4e0d9aad1a0a8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spec shape: torch.Size([16, 129, 2])\n",
      "label shape: torch.Size([16])\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Now we are approaching training, meaning model will se annotated examples and will learn to recognize if mass spectra contain chlorine \n",
    "\n",
    "#### Can take some time, however, once the DreaMS is trained, you do not need to redo all these steps, but just load already learned DreaMS"
   ],
   "id": "2610bbb740a0144a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TODO add condition if GPU available",
   "id": "fc528ef9ea579008"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# trainer = pl.Trainer(\n",
    "#     max_epochs=5,\n",
    "#     accelerator=\"cpu\",  # or \"gpu\"\n",
    "#     devices=1,\n",
    "#     log_every_n_steps=10,\n",
    "# )"
   ],
   "id": "f003e5944757d5c9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:21:37.389837Z",
     "start_time": "2025-07-05T09:21:37.313806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=1,\n",
    "    limit_train_batches=5,     # run only 5 training batches\n",
    "    limit_val_batches=3,       # run only 3 validation batches\n",
    "    limit_test_batches=3,      # run only 3 test batches\n",
    "    accelerator=\"cpu\",         # or \"gpu\"\n",
    "    devices=1,\n",
    ")"
   ],
   "id": "54e9400cd8045bfe",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:75: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:22:02.028092Z",
     "start_time": "2025-07-05T09:21:48.102238Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.fit(lit, datamodule=dm_dreams)",
   "id": "ffe28494d3e4566a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | model     | DreamsClassifier | 95.6 M\n",
      "1 | train_acc | BinaryAccuracy   | 0     \n",
      "2 | val_acc   | BinaryAccuracy   | 0     \n",
      "3 | val_auc   | BinaryAUROC      | 0     \n",
      "-----------------------------------------------\n",
      "95.6 M    Trainable params\n",
      "0         Non-trainable params\n",
      "95.6 M    Total params\n",
      "382.202   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f27269916e34550aec4101772e10c1c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: No positive samples in targets, true positive value should be meaningless. Returning zero tensor in true positive score\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c6140e69cbc04bfa857309a482112fef"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8ef774edae0478ead276cb305a0b8af"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:22:03.867446Z",
     "start_time": "2025-07-05T09:22:02.089676Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.test(lit, datamodule=dm_dreams)",
   "id": "ab9cde0d75ddae2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8f77ef73d3de4997a32dfddfda959253"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "        test_acc                    1.0\n",
      "        test_loss          7.947286206899662e-08\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 7.947286206899662e-08, 'test_acc': 1.0}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Now model was sucessfuly trained and we do not want to repeat computationally expensive training, so we save learned model and will reuse it ",
   "id": "d7a05a587cb51f17"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T09:31:31.574184Z",
     "start_time": "2025-07-05T09:31:30.367756Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 4) Save your fine-tuned checkpoint\n",
    "trainer.save_checkpoint(\"dreams_chlorine_finetuned.ckpt\")"
   ],
   "id": "987cddd8074da8ad",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 4 – Reload & verify",
   "id": "edf67f838dec272b"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-07-05T09:31:51.812657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "lit2 = LitDreamsClassifier.load_from_checkpoint(\"dreams_chlorine_finetuned.ckpt\")\n",
    "lit2.eval()\n",
    "test_trainer = pl.Trainer(accelerator=\"cpu\", devices=1)\n",
    "test_results = test_trainer.test(lit2, datamodule=dm_dreams)\n",
    "print(test_results)"
   ],
   "id": "d4af7d21bd7845f5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/macbook/UTILS/anaconda3/envs/dreams_mimb/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ea56400352c4786a047850d6e56ddfd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8159592627a2081c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
